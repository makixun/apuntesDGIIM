%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Documento con ejercicios resueltos de la
% asignatura Análisis Matemático I.
% Doble Grado de Informática y Matemáticas.
% Universidad de Granada.
% Curso 2016/17.
%
% Licencia:
% CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%--------------------------------------------
%   Topología de un espacio métrico
%--------------------------------------------
\section{Topología de un espacio métrico.}

\begin{ejer}
Dado el conjunto $A = \{ (x,y)\in \mathbb{R}^2: 0 < x \le 1 \}$, ¿es abierto?

\begin{proof}
Tenemos que comprobar si $A$ es abierto, es decir, si es cierto que\\
$\forall a \in A \ \ \exists s>0\ tal\ que\ B(a,s)\subset A$. Para ello, fijo $y \in \mathbb{R}$ y escojo $x_o = (1,y) \in A$. Además, tomo $s>0$ cualquiera. Veamos que hay puntos $z \in B(x_o, s)$ que no pertenecen a $A$.

Sea $\displaystyle z = (1 + \frac{s}{2}, y)$. Entonces, se tiene que $\displaystyle d(z,x_o)= \sqrt{\left(\frac{s}{2}\right)^2 + 0} = \frac{s}{2} < s$, y por tanto $z \in B(x_o,s)$.

Claramente $z\notin A$, pues $\displaystyle 1 + \frac{s}{2} > 1$. Así, concluimos que $z$ es un punto de $B(x_o,s)$ que no pertenece a $A$, por lo que $\displaystyle B(x_o,s) \not \subseteq A$, y $A$ no es abierto. \qedhere

%\textbf{@ Antonio Coín.}
\end{proof}

\end{ejer}

\section{Conexión.}
\begin{ejer} Dado el conjunto \emph{peine} de la siguiente forma, probar que es conexo:
\[
	P = (\{ 0\} \times (0,1]) \cup ((0,1] \times \{0\}) \cup (\{\frac{1}{n} : n \in \mathbb{N}\} \times (0,1))
\]

\begin{center}
\includegraphics[scale=0.4]{./img/peine.png}
%\includegraphics[scale=0.4]{./Análisis Matemático I/imagenes/peine.png}
%% Descomentando la línea anterior se puede generar el pdf con el rakefile
%% Dejo sin comentar la primera porque normalmente trabajamos con el .tex desde la carpeta de Análisis
\end{center}

\begin{proof}
Para la demostración vamos a definir el siguiente conjunto:
\[
	S = ((0,1] \times \{0\}) \cup (\{\frac{1}{n} : n \in \mathbb{N}\} \times (0,1))
\]
Es decir, el mismo conjunto P quitandole el eje y. Siendo $\bar{S} = P \cup (0,0)$.
De esta forma tenemos:
\[
S \subset P \subset \bar{S}
\]

Para demostrar este ejercicio utilizaremos la siguiente proposición:\\

\begin{nprop}
Sea A un conjunto conexo, y B tal que $A \subset B \subset \bar{A} \implies B$ es conexo.
\end{nprop}

\begin{proof}
 Para la demostración probaremos su contrarecíproco, consistente en: B no es conexo $\implies$ A no es conexo.

B no es conexo $\implies \exists O,O'$ abiertos $: \begin{cases}
B \subset O\cup O'\\
O \cap B \cap O' = \emptyset\\
O \cap B \neq \emptyset \neq O' \cap B\\
\end{cases}$

 Debemos probar que los 3 resultados se cumplen para A.

 1. $B \subset O \cup O' \implies A \subset B \subset O \cup O'$\\
 2. $B \cap (O\cap O') = \emptyset \implies \nexists x \in B$ tq $x \in O \cap O'$. \\
 Entonces $\forall x \in A \implies x \in B \implies x \not\in O \cap O'$\\
 3. $O \cap B \neq \emptyset \implies O\cap \bar{A} \neq \emptyset \implies O\cap (A\cup fr(A) \neq \emptyset \implies (O\cap A)\cup (O\cap fr(A)) \neq \emptyset$
 En este punto tenemos 3 posibilidades:\\
  - El primer elemento de la union es distinto del vacio, en cuyo caso hemos acabado.\\
  - Ambos elementos son distintos del vacio, en cuyo caso tambien queda demostrado.\\
  - El segundo elemento de la unión es distinto del vacio. $\exists x \in O\cap fr(A) \implies \exists \epsilon > 0 : B(x,\epsilon) \in O$. Por estar en la frontera: $B(x,\epsilon)\cap A \neq \emptyset \implies \exists y \in B(x,\epsilon)\cap A \subset A \cap O$.\\\\

  Nota: La idea de esta demostración es que: dado un conjunto conexo, su unión con elementos que se encuentran cerca de el, sigue siendo conexa.

\end{proof}

Teniendo esto demostrado, haciendo uso de la proposición y de que S es conexo (por ser arco-conexo). Queda demostrado que P es conexo.

\end{proof}

\end{ejer}


\begin{ejer}
	Estudiar los máximos y mínimos relativos de la función $f: R^2 \to R$ dada por:
	\begin{enumerate}

	\item $f(x,y)=x^2+y^2$. Tenemos que estudiar los puntos críticos de f. Para ello, estudiamos:
	\[
	\begin{rcases}
	\frac{\partial f}{\partial x}(x,y) = 0\\
	\frac{\partial f}{\partial y}(x,y) = 0
\end{rcases} \implies \begin{rcases}
	2x= 0\\
	2y = 0\\
\end{rcases}
	\]
	Ahí podemos obtener los puntos críticos, que como vemos es el (0,0). Ahora, tenemos que ver si es un máximo o un mínimo usando la matriz Hessiana, que se calcula derivando dos veces por cada variable.[REVISAR CÓMO SE OBTENDRÍA LA HESSIANA]

	Calculamos la matriz Hessiana y la evaluamos en el (0,0), el punto obtenido:
	\[
	Hf(x,y) = \begin{pmatrix} 2 & 0 \\ 0 & 2 \end{pmatrix}
	\]
	Por tanto en el (0,0) también es esa matriz y queda ver que es definida positiva, pero eso es trivial por los determinante encajados. Por tanto, $f$ tiene un mínimo local en 0.

	\item $f(x,y) = -x^2-y^2$. Este apartado es igual, pero la matriz tendrá -2 en vez de 2 y por ello tendrá un máximo.

	\item $f(x,y) = x^2-y^2$. Estudiamos puntos críticos:
	\[
	\begin{rcases}
	\frac{\partial f}{\partial x}(x,y) = 0\\
	\frac{\partial f}{\partial y}(x,y) = 0
\end{rcases} \implies \begin{rcases}
	2x= 0\\
	-2y = 0\\
\end{rcases}
	\]

	Por tanto, su matriz hessiana es:
		\[
	Hf(x,y) = \begin{pmatrix} 2 & 0 \\ 0 & -2 \end{pmatrix}
	\]

	Y esta matriz no es ni semidefinida positiva ni semidefinida negativa. Por tanto, en este caso el (0,0) no es ni máximo ni mínimo relativo.
	Como no es ni máximo ni crítico pero su derivada es cero, se denomina punto de silla.

	\item $f(x,y)=x^2-xy+y^2$ Estudiamos puntos críticos:
	\[
	\begin{rcases}
	\frac{\partial f}{\partial x}(x,y) = 0\\
	\frac{\partial f}{\partial y}(x,y) = 0
\end{rcases} \implies \begin{rcases}
	2x -y= 0\\
	2y-x = 0\\
\end{rcases}
	\]

	Resolvemos el sistema, y nos queda la solución (0,0) también como solución única.

	Por tanto, su matriz hessiana es:
		\[
	Hf(x,y) = \begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}
	\]

	Como los determinantes encajados son positivos, la matriz hessiana es definida positiva por tanto (0,0) es un mínimo


\end{enumerate}
\end{ejer}

\begin{ejer}[5 relación derivabilidad]
	Sea $\Omega \subset \R^n$ abierto y $f:\Omega \to \R^n$ y $a\in \Omega$ con $Df(a) = 0$ ( es decir, $\triangledown f(a) = 0$) con $def(Hess f(a)) \ne 0$.

	Probar que $\exists U$ abierto en $\Omega$ tal que $a \in U$ tal que:
	\[
	\begin{rcases}
	Df(x) = 0\\
	x \in U
\end{rcases} \iff x = a
	\]
\end{ejer}
\begin{proof}[Solución]
	Calculamos su hessiana:
	\[
	Hf(a) = \begin{pmatrix} \frac{\partial^2f(a)}{\partial^2x_1^2}& \cdots & \frac{\partial^2f(a)}{\partial x_1 \partial x_n} \\ \cdots & & \cdots \\ \frac{\partial^2f(a)}{\partial x_n \partial x_1}& \cdots & \frac{\partial^2f(a)}{\partial^2x_n^2} \end{pmatrix} = Jg(a) = J(\triangledown f)(a)
	\]
Y tenemos que $g:\Omega \to \R^n$, $g(x) = \triangledown f(x) = ( \frac{\partial f(x)}{\partial x_1}, \cdots , \frac{\partial f(x)}{\partial x_n})$.

Ahora, como el determinante de la Hessiana de f en a es distinto de cero, por el Teorema de la función inversa aplicado a $g =  \triangledown f$, entonces $\exists U$ abierto en $\Omega$, $a\in U$ y también $\exists V$ abierto en $\R^n$ con $g(a) =  \triangledown f(a) \in V$
tales que:

$g = \triangledown f: U \to V$ biyectiva que lleva $a \mapsto g(a) =  \triangledown f(a) = 0$


\end{proof}
\begin{ejer}
	Sea $f:\R^2 \to \R^2$ dada por $f(x,y) = (e^x cos(y),e^x sen(y)) = e^x(cosy,seny) \in \partial B(0,e^x)$. Probar que $f$ es localmente invertible en todo punto de $\R^2$
\end{ejer}
\begin{proof}[Solución]
	Primero, es fácil ver que $f$ no es inyectiva en todo $\R^2,$pues basta ver que $f(x,y+2\pi) = f(x,y)$

	Lo único que tendríamos que hacer es ver que el determinante de $Jf (x_0,y_0)$.
	\[
det \begin{pmatrix} e^x cosy & -e^xseny \\ e^x seny & e^x cos y \end{pmatrix}  = e^x \ne 0
	\]
	Y como no es 0, la función es localmente invertible en el punto $x_0,y_0$ por el Teorema de la función inversa.

\end{proof}
\begin{ejer}[6 de la relación]

\[
\begin{rcases}
	x^2 - y^2 -u^3 +v^4 = -4\\
	2xy + y^2 -2u^2 -3v^2 = -8
\end{rcases} = \begin{rcases}
	x^2 - y^2 -u^3 +v^4  + 4= 0\\
	2xy + y^2 -2u^2 -3v^2  + 8=  0
\end{rcases}
\]
	Podemos considerarla como la función $F: \R^4 \to \R^2$ dada por\\
	 $F(x,y,u,v) = (x^2 - y^2 -u^3 +v^4  + 4,2xy + y^2 -2u^2 -3v^2  + 8)$
	 Y podemos ver la ecuación como $F(x,y,u,v) = (0,0)$.
	 Tendremos que usar dos variables como parámetros, pues necesitaríamos 4 variables para resolver el sistema.
	 Hacemos entonces la jacobiana de esta función:

	 \[
	 det\begin{pmatrix}
 2x & -2y \\
 2y & 2x+2y
\end{pmatrix}
	 \]
	 Y habría que evaluar el determinante en el punto que nosotros habíamos tomado, que es una solución particular que sería : $x=2, y = 1,u = 2,v = 1$, por lo que sería ver el determinante de:
	 \[
	 \begin{pmatrix}
 4 & 2 \\
 -2 & 0
\end{pmatrix}
	 \]
, que es distinto de 0, por lo que por el teorema de la función implícita, existen U entorno abierto de (2,-1) en $\R^2$ y existe V entorno abierto de (2,1) en $\R^2$ tales que:
\begin{enumerate}
	\item UxV$\subset A = \R^2$
	\item $\forall(u,v) \in V \ \ \exists ! (x,y) \in U : F(x,y,u,v) = (0,0)$
\end{enumerate}
\end{ejer}



\begin{ejer}
	Sea $f:\R^2 \to \R$
	$f(x,y) = 2x^2 -y^2 +4x+2y+1$.

	\textbf{Procedimiento}: Para estudiar los extremos relativos hacemos puntos criticos y eso nos lleva a un conjunto de puntos que tendremos que estudiar. Luego,si estudiamos el máximo o el mínimo absoluto en un compacto, tendremos probar que el máximo y el mínimo se alcanzan (T.W.) y sabiendo que son máximo o mínimos absolutos, si el punto estuviera en el interior sería un máximo o mínimo relativo luego tenemos que estudiar los puntos de los bordes estudiando las funciones que están definidas en un abierto(IMPORTANTE). Cuando tengamos los puntos que puedan ser puntos críticos, calculamos su valor por la función y afirmamos cuál es el máximo y cuál es el mínimo.\\

\textbf{Apartado a:}\\
	 Estudiar extremos relativos y puntos de silla de f.

	Los puntos criticos de f son los puntos en los que el gradiente de f vale 0 ( $\triangledown f (x,y) = (0,0)0)$. Calculamos derivadas:
	\[
	\frac{\partial f}{\partial x}(x,y) = 4 x+ 4
	\]\[
	\frac{\partial f}{\partial y}(x,y) = -2y +2
	\]
	Por lo que tenemos que calcular las soluciones del sistema:
	\[
	\begin{rcases}
	4x+4 = 0\\
	-2y+2 = 0
\end{rcases}
	\]
	Cuya solución es el punto (-1,1).
	Estudiamos la matriz hessiana en ese punto para ver qué es.

	\[
	Hesf(x,y) = \begin{pmatrix}
 \frac{\partial^2f}{\partial x^2}(x,y) &  \frac{\partial^2f}{\partial x \partial y}(x,y) \\
  \frac{\partial^2f}{\partial x \partial y}(x,y) &  \frac{\partial^2f}{\partial y^2}(x,y)
\end{pmatrix}  =  \begin{pmatrix}
  4& 0 \\
 0 & -2
\end{pmatrix}
	\]
	Y el determinante de esa es -8 y además el primer determinante encajado es positivo, luego esa matriz no es ni definida positiva ni definida negativa luego tenemos que en (-1,1) hay un punto de silla (esta es su definición).

	\textbf{Apartado b:}

	Si llamamos T= triángulo cerrado de vértices (-3,0),(0,0) y (0,3) que es cerrado y acotado. Este T es compacto por ser cerrado y acotado por tanto, por el teorema de Weierstrass f alcanza un máximo valor y un mínimo en puntos de T.
	Esto es un problema de extremos condicionados.

	Si estuvieran los puntos críticos que hemos encontrado antes dentro del triángulo, entonces serían extremos relativos y hemos visto antes que no tiene. Luego el máximo y mínimo absoluto de f en T no puede ser un punto interior a T. Hay que estudiarlos en la frontera de T.

	La frontera de T está constituida por los 3 segmentos que unen los 3 puntos del triángulo y los puntos, es decir, "6 partes". Esto es:
	\[
	\partial T = A_1 \cup A_2 \cup A_3 \cup (0,3) \cup (-3,0) \cup (0,0)
	\]
	Siendo:\\ $A_1 = \{ (0,y) : 0 < y < 3\}  = \{(x,y) \in (-3,0)x\R : x = 0\}$,\\
	 $A_2 =\{(x,0) :  -3 < x < 0\} = \{(x,y) \in (-3,0)x \R : y = 0\}$  \\
	 $A_3 =\{(x,x+3) :  -3< x < 0\} = \{(x,y)\in (-3,0)x\R :  y -x-3 = 0\}$.\\

	Ahora, al descomponerlo así podemos usar en cada fragmento el teorema de lso multiplicadores de Lagrange.

	Empezamos por $A_3$ . Definimos $g(x,y) = y-x-3$ y sabemos que los posibles extremos condicionados serían extremos de la Lagrangiana es decir, tienen que ser puntos críticos de la función: $f+\lambda g$ dependiendo de $\lambda$.
	Esta función es:
	\[
	F(x,y,\lambda) = f(x,y) + \lambda g(x,y)
	\]
	E igualamos de nuevo el gradiente de esta función a 0:
	\[
	\frac{\partial F}{\partial x} =  \frac{\partial F}{\partial y} =\frac{\partial F}{\partial \lambda} = 0
	\]
	Obteniendo el sistema:
	\[
	\begin{rcases}
	4x +4 - \lambda = 0\\
	-2y +2 + \lambda = 0\\
	 y-x-3 = 0
\end{rcases}\implies  \begin{rcases}
	x = 0\\
	y = 3 \\
\end{rcases}
	\]
	Pero el punto (0,3) no está en $A_3$, es un punto aparte luego no hay ningún extremo condicionado a la restricción de pertenecer a $A_3$. Si existiera un extremo relativo en ese segmento también sería extremo relativo sobre ese segmento, pero como hemos visto que no tiene, no puede haber extremo relativo.

	Hacemos lo mismo con $A_2$. En este caso $g(x,y) = y$ Nos queda el sistema:
	\[
	\begin{rcases}
	4x +4  = 0\\
	-2y +2 + \lambda = 0\\
	 y = 0
\end{rcases}\implies \begin{rcases}
	x=-1\\
	y = 0\\
	\lambda = -2
\end{rcases}
	\]
	Tenemos el punt (-1,0) que sí pertenece a $A_2$, por tanto este punto puede ser extremo relativo condicionado por la condición $y = 0$

	Repetimos el proceso para $A_1$. En este caso, $g(x,y) = x$. Volvemos a hacer el sistema y nos queda:
	\[
	\begin{rcases}
	4x +4 +\lambda = 0\\
	-2y +2  = 0\\
	 x = 0
\end{rcases}\implies \begin{rcases}
	x=0\\
	y = 1\\
	\lambda = -4
\end{rcases}
	\]
	Tenemos el punto(0,1) que sí está en este segmento.

	 Todo esto sirve para afirmar que el máximo y el mínimo de f en T es uno de estos puntos:
	\[
	(0,1),(-1,0),(0,0),(0,3),(-3,0)
	\]
	Ahora, sabiendo esto calculamos la f en cada uno de estos puntos y vemos cuáles son los extremos y hemos acabado.


\end{ejer}


\begin{ejer}
	Tomemos la gráfica  $G\{(x,1/x): x > 0\}$ (gráfica de la función $y= 1/x$) .Probar que existe un punto en G tal que
\[
d((0,0),(x,1/x) = Min\{d((0,0),(y,1/y) : y > 0\}
\]

Para ello, debemos definir
\[
f(y) = d((0,0),(x,1/x)) \ \ continua
\]
Si nos enfrentaramos a un ejercicio donde el conjunto no es compacto, un "truco" a seguir es coger un punto del conjunto al azar, y trazar la bola cerrada de distancia la del (0,0) a ese punto. Al tomar la interseccion entre el conjunto y esa bola cerrada, tenemos un conjunto cerrado, que nos aseguramos que contiene el mínimo.
\end{ejer}


\begin{ejer}[26 de la relacion de derivabilidad]
	Sea $\Omega \subset \R^n$ abierto y acotado . Sea $u \in \varphi(\bar{\Omega} \cup \varphi^2(\Omega)$.

	Apartado a:

	$\triangle u(x):= \sum_i^n \frac{\partial^u}{\partial x_i^2}(x) < 0 \ \forall x \in \Omega$ es la traza. Entonces, u no puede alcanzar su mínimo absoluto en $\bar{\Omega}$ en un punto de $\Omega$.

Nota:
	$\Omega \subset \R^n$ abierto y acotado $\implies \begin{rcases}
	cerrado \ + \ acotado = \ compacto \\
	u \in \mathcal{C}(\bar{\Omega})
\end{rcases}\implies $ u alcanza su maximo y minimo en $\bar{\Omega}$


\begin{proof}[Solución apartado a:] \textbf{Este ejercicio es trivial.}\\
	Supongamos que fuese falso, es decir:supongamos que el mínimo absoluto de $u$ en $\bar{\Omega}$ se alcanza en $a\in \Omega$ : $u(a) = min\{u(x): x \in \bar{\Omega}\}$, esto implica que $\exists r > 0 : B(a,r) \subset \Omega $ y $u(x) \geq u(a) \forall x \in B(a,r) \implies a$ es un mínimo relativo de u, por lo que:
	\[
	\frac{\partial u}{\partial x_i}(a) = 0 \quad \forall i = 1,...,N
	\]
	\[
	\frac{\partial^2 u}{\partial x_i^2}(a) \geq 0 \implies \sum_i^n \frac{\partial^2u}{\partial x_i^2} \geq 0
	\]
	Y esto es una contradicción.
\end{proof}

Apartado b:

Si el $\triangle u(x) \leq 0 \quad \forall x \in \Omega \implies (min)_{x\in \bar{\Omega}} \ \ u(x) = (min)_{x \in \partial \Omega}  \ \ u(x)$.


\end{ejer}

\begin{ejer}[14 de la relación de derivabilidad]\hfill\\
	Sea $f: \R^2 \to \R^2$ con $f(x,y) = (y+cosx,x+e^y)$\\
	Sea $g: \R^2 \to \R$ con $g(t,u) = t+u$
	\begin{nlist}
	\item Calcular la matriz jacobiana de $F=gof$
	\item Calcular la derivada direccional de $F$ en el punto $(0,0)$ a lo largo de la dirección $(2,-1)$
	\item Calcular el valor máximo de la derivada direccional de $F$ en $(0,0)$
	\item $F(x,y) = 2$ define a y como una función implícita diferenciable $\mathcal{C}^\infty$ en un entorno de (0,0).
\end{nlist}
\end{ejer}
\begin{proof}[Solución]
	\begin{nlist}
	\item $F=g(f(x,y)) = g(y+cosx,x+e^y) = y+cosx+x+e^y$.

	Lo haremos usando la regla de la cadena. $DF(x,y) = Jg(f(x,y))Jf(x,y)$
	\[
	Jf(x,y)= \begin{pmatrix}
 -senx & 1 \\
 1 & e^y
\end{pmatrix} = Jg(y+cosx,x+e^y)
	\]
	\[
	Jg(f(x,y))=Jg(y+cosx,x+e^y) = (1,1)
	\]
	\[
	Jg(f(x,y))Jf(x,y) = (1 \ \ \ 1)\begin{pmatrix}
 -senx & 1 \\
 1 & e^y
\end{pmatrix} = (1- senx \ \ \ 1+e^y)
	\]

	\item Vamos a calcular primero el vector unitario de (2,-1). $|(2,-1)| = 5 \implies (2/\sqrt5, -1/ \sqrt5)$. Ahora, tenemos
	\[
	D_{ (2/\sqrt5, -1/ \sqrt5)}F(0,0) = JF(0,0) \begin{pmatrix}
 2/\sqrt 5  \\
 -1/\sqrt5
\end{pmatrix} = 0
	\]

	\item \[
	D_vF(0,0) = JF(0,0) v =  \triangledown F(0,0) v \leq \sqrt 5 \ \ \ \forall |v| = 1	\]
	Es una cota superior por el lema de cauchy schwarz.
	Ahora, \[
	v = \frac{\triangledown F(0,0)}{|\triangledown F(0,0)|} \implies D_v F(0,0) = \triangledown F(0,0)  \frac{\triangledown F(0,0)}{|\triangledown F(0,0)|} = |\triangledown F(0,0)| = \sqrt 5
	\]

	Podemos obtener como un resultado o propiedad:

	\begin{nprop}
	Si $\triangledown F(a,b) \ne (0,0)$ entonces:
	\[
	D_{ \frac{-\triangledown F(a,b)}{|\triangledown F(a,b)|}}F(a,b) =  |\triangledown F(a,b)| \leq D_v F(a,b) \leq |\triangledown F(a,b)| = D_{ \frac{\triangledown F(a,b)}{|\triangledown F(a,b)|}}F(a,b) \ \ \ \forall |v| = 1
	\]
\end{nprop}

Otra forma de haber hecho el ejercicio era mediante extremos condicionados:
\[
(1 \ \ 2) \begin{pmatrix}
 v_1 \\
 v_2
\end{pmatrix}
\]

\item Lo haremos pensando en la función $H(x,y) =F(x,y)-2 = 0$, pues nos ayudará para usar el teorema de la función implicita. Ahora, la función H es de clase infinito pues es composición de dos funciones que son de clase infinito, por lo que podemos usar el teorema. Tenemos que comprobar que $H(0,0) = 0$, que se comprueba fácilmente.
\[
JH(x,y) = JF(x,y) = \begin{pmatrix}
 1-senx & e^y+1 \\

\end{pmatrix}
\]
\[
JH(0,0)= \begin{pmatrix}
 1 & 2 \\
\end{pmatrix}
\]
Ahora, tenemos que aclarar cuál de los menores vamos a coger de la matriz (1   2), pues pueden ser la x o la y las funciones implícitas. Para ver cual cogeremos, usar una regla pnemotécnica.

Calcular $y'(0)$ e $y''(0)$ con y la función implícita una vez que la hemos calculado.
\end{nlist}
\end{proof}

\begin{ejer}[14 de la relación de derivabilidad]
Sea $F:\mathbb{R}^2 \longrightarrow \mathbb{R}$ la función dada por $F(x,y) = y + cosx + x + e^y \quad \forall(x,y)\in \mathbb{R}^2$.

\textbf{Apartado d)}. Probar que $F(x,y)=2$ define a $y$ como función implícita diferenciable $C^{\infty}$ en un entorno de $(0,0)$. Además, calcular $y'(0)$ e $y''(0)$.

\begin{proof}[Solución]
Definimos $H:\mathbb{R}^2 \longrightarrow \mathbb{R}$ dada por $H(y,x)=F(y,x)-2$.

\textbf{Observación 1.} $H$ es $C^{\infty}$, por ser suma de funciones $C^{\infty}$.\\
\textbf{Observación 2.} $H(0,0) = 0 + cos0 + 0 + e^0 - 2 = 0$. Esto es condición indispensable para poder aplicar el \textit{teorema de la función implícita}.\\
\textbf{Observación 3.} Vemos el conjunto de definición de $H$ como $\mathbb{R}^2 = \mathbb{R} \times \mathbb{R}$, donde la primera componente es "\textit{el parámetro"} ($y$), y la segunda componente es \textit{"la variable"} ($x$).

El siguiente paso es comprobar que $\displaystyle \det \begin{pmatrix}
	\pder[H]{y}(0,0)\\
\end{pmatrix} \ne 0$:

$$\pder[H]{y}(y,x) = 1 + e^y;\quad \det \begin{pmatrix}
	\pder[H]{y}(0,0)\\
\end{pmatrix} = \det \left( 1 + e^0 \right) = \det (2) = 2 \ne 0.$$

Ahora, estamos en las condiciones de aplicar el \textbf{teorema de la función implícita} a $H$:
$\exists U$ entorno abierto de $0$ en $\mathbb{R}$, $\exists V$ entorno abierto de $0$ en $\mathbb{R}$, tal que:

\begin{enumerate}
\item $U\times V \subseteq \mathbb{R}^2$
\item $\forall x \in V \ \exists ! \ y \in U\ tal\ que \ H(y,x) = 0$
\end{enumerate}

Es decir, podemos definir la función $y:V \longrightarrow U$ que a cada $x\in V$ le hace corresponder un $y(x)\in U$, de tal forma que $H(y(x),x) = 0$. Además, el teorema garantiza que $y$ es $C^{\infty}$ por serlo $H$. Por tanto, hemos probado que $F(x,y)=2$ define a $y$ como función implícita $C^{\infty}$ en un entorno de $(0,0)$.

\underline{Calculamos ahora $y'(0)$}. Sabemos que $y(0)=0$, pues $H(0,0) = 0$. También sabemos que, dado $x\in V$, la pareja $(y(x),x)$ es solución de la ecuación $H(y,x)=0$. Por tanto, se tiene que:
\vspace{0.5em}
$$y(x) + cosx + x + e^{y(x)} - 2 = 0$$

Derivamos ahora la función con respecto a "la variable" $x$, obteniendo:
\vspace{0.5em}
$$y'(x) - senx + 1 + e^{y(x)}\cdot y'(x) = 0$$

Por último, evaluamos en el punto $x=0$, que sabemos que es solución:
$$y'(0) - sen0 + 1 + e^{y(0)}\cdot y'(0) = 0 \Rightarrow y'(0) + 1 + y'(0) = 0 \Rightarrow \bm{y'(0) = \frac{-1}{2}}$$

\underline{Calculamos ahora $y''(0)$}. Sabiendo que $y'(0) = -1/2$, volvemos a derivar la expresión anterior, obteniendo:
\vspace{0.5em}
$$y''(x) - cosx + e^{y(x)}\cdot y''(x) + e^{y(x)}\cdot y'(x)= 0$$

Por último, evaluamos en el punto $x=0$, que sabemos que es solución:
$$y''(0) - cos0 + e^{y(0)}\left( y''(0) + y'(0) \right) = 0 \Rightarrow y''(0) - 1 + y''(0) - \frac{1}{2} = 0 \Rightarrow \bm{y''(0) = \frac{3}{4}}$$
\end{proof}
\end{ejer}

\begin{ejer}[Función lipschitziana respecto de una variable] \hfill

En orden a probar el \textbf{teorema de Ascoli-Arzelá} (ó \textit{teorema de Picard-Lindelöf}) necesitábamos verificar que una cierta función $f$ cumplía la condición de Lipschitz respecto de su segunda variable, bajo la hipótesis de que $f$ era $C^1$. Sin embargo, basta que $f$ cumpla una condición menos restrictiva, como veremos.

\textbf{Proposición.} Sea $f:\mathbb{R}^2 \longrightarrow \mathbb{R}$ una función continua, derivable respecto de su segunda variable, y verificando que $\pder[f]{x}(s,x)$ es continua. Sean $a,b \in \mathbb{R}$ con $a < b$, y consideremos dos funciones $y,z \in C([a,b],\mathbb{R})$. Entonces, se tiene que: $$\exists K > 0 : \ \ |f(s,y(s)) - f(s,z(s)) | \le K|y(s)-z(s)| \ \ \forall s \in [a,b]$$

\begin{proof} \hfill

Para probar el resultado, podemos considerar sin pérdida de generalidad que $f$ está definida en el conjunto $[a,b] \times \mathbb{R}$. Además, como $y,z$ son funciones continuas definidas en un compacto, sabemos que sus imágenes son compactas. Así, podemos definir $A = y([a,b]) \cup z([a,b])$, y afirmamos que $A$ es compacto, pues es unión finita de compactos. Por tanto, existirán números reales $c$ y $d$ tales que $c = \text{mín} A$ y $d = m\acute{a}x A$, con $c \le d$. Entonces, podemos restringir aún más el conjunto de definición de $f$, y consideramos que está definida en el compacto $D = [a,b]\times [c,d]$.

Ahora, como $f$ es $C^1$ respecto de su segunda variable, tenemos que la función $\displaystyle\pder[f]{x}: D \longrightarrow \mathbb{R}$ es una función continua definida en un compacto. Por tanto, su imagen está acotada, y esto nos permite definir: $$M := sup \left\{ \left| \pder[f]{x}(s,x) \right|\colon (s,x) \in D\right\} \ge 0$$

Fijamos $s \in [a,b]$, y consideramos la función $f_s(x):[c,d] \longrightarrow \mathbb{R}$ dada por $f_s(x) = f(s,x)$.\\ Es inmediato comprobar que $f_s$ es $C^1$, pues $f_s'(x) = \pder[f]{x}(s,x)$. Observamos también que si $y(s)=z(s)$, la proposición se cumple trivialmente para cualquier valor de $K$, por lo que podemos suponer $y(s) < z(s)$ (en otro caso, intercambiamos sus papeles). Entonces, por el \textbf{teorema del valor medio} aplicado a $f_s$, tenemos que $\exists c \in [y(s), z(s)]$ verificando: $$|f_s(y(s)) - f_s(z(s))| = |f'_s(c)|\cdot|y(s)-z(s)|$$

De esta igualdad concluímos que: $$|f(s,y(s)) - f(s,z(s))| = \left|\pder[f]{x}(s,c)\right|\cdot|y(s)-z(s)| \le M\cdot|y(s)-z(s)|$$

Notemos que si $M=0$, sería $f'_s(x) = 0 \ \forall x \in [c,d]$, por lo que $f_s$ sería constante, y la proposición sería cierta para todo $K$ positivo. Es por esto que podemos suponer $M > 0$. Como $s$ era un punto arbitrario de $[a,b]$, queda probado el resultado, donde elegimos $K=M$.
\end{proof}

\textbf{Nota.} En el caso en que $f \in C^1(\mathbb{R}^2,\mathbb{R})$, podemos aplicar directamente el resultado anterior, pues sabemos que en particular $f$ es continua, y derivable respecto de su segunda variable con derivada continua.

\end{ejer}
